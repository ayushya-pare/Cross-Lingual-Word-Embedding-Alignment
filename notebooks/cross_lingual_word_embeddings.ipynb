{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download English model (.bin format)\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # Downloads cc.en.300.bin\n",
    "ft_en = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# Save top 100,000 English words to .vec file\n",
    "top_k = 100000\n",
    "en_words = ft_en.get_words()[:top_k]\n",
    "\n",
    "with open('en_top100k.vec', 'w', encoding='utf-8') as f:\n",
    "    for word in en_words:\n",
    "        vector = ft_en.get_word_vector(word)\n",
    "        vector_str = ' '.join(map(str, vector))\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Hindi model\n",
    "fasttext.util.download_model('hi', if_exists='ignore')  # Downloads cc.hi.300.bin\n",
    "ft_hi = fasttext.load_model('cc.hi.300.bin')\n",
    "\n",
    "# Save top 100,000 Hindi words to .vec file\n",
    "top_k = 100000\n",
    "hi_words = ft_hi.get_words()[:top_k]\n",
    "\n",
    "with open('hi_top100k.vec', 'w', encoding='utf-8') as f:\n",
    "    for word in hi_words:\n",
    "        vector = ft_hi.get_word_vector(word)\n",
    "        vector_str = ' '.join(map(str, vector))\n",
    "        f.write(f\"{word} {vector_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load English and Hindi Vocab Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create English vocab set\n",
    "en_vocab = set()\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/fasttext_pretrained_vectors/en_top100k.vec', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            word = parts[0]\n",
    "            en_vocab.add(word)\n",
    "\n",
    "# Create Hindi vocab set\n",
    "hi_vocab = set()\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/fasttext_pretrained_vectors/hi_top100k.vec', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) > 1:\n",
    "            word = parts[0]\n",
    "            hi_vocab.add(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Filter the MUSE Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to original MUSE bilingual dictionary\n",
    "muse_dict_path = '/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/muse_bilingual_dictionary/en-hi.txt'  # Update if needed\n",
    "\n",
    "# Save only valid pairs to this file\n",
    "filtered_dict_path = '/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/muse_bilingual_dictionary/valid_pairs.txt'\n",
    "\n",
    "with open(muse_dict_path, 'r', encoding='utf-8') as f_in, open(filtered_dict_path, 'w', encoding='utf-8') as f_out:\n",
    "    for line in f_in:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        en_word, hi_word = parts[0], parts[1]\n",
    "        if en_word in en_vocab and hi_word in hi_vocab:\n",
    "            f_out.write(f\"{en_word} {hi_word}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Valid Pairs into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all valid pairs into a list\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/muse_bilingual_dictionary/valid_pairs.txt', 'r', encoding='utf-8') as f:\n",
    "    all_pairs = [line.strip().split() for line in f if len(line.strip().split()) == 2]\n",
    "\n",
    "# Shuffle the list (important for randomness)\n",
    "import random\n",
    "random.shuffle(all_pairs)\n",
    "\n",
    "# Split into train (80%) and test (20%)\n",
    "split_index = int(0.8 * len(all_pairs))\n",
    "train_pairs = all_pairs[:split_index]\n",
    "test_pairs = all_pairs[split_index:]\n",
    "\n",
    "# Save them to separate files\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/train_pairs.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, hi in train_pairs:\n",
    "        f.write(f\"{en} {hi}\\n\")\n",
    "\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/test_pairs.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, hi in test_pairs:\n",
    "        f.write(f\"{en} {hi}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Load Embeddings Again for Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English embeddings into a dictionary\n",
    "en_vectors = {}\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/fasttext_pretrained_vectors/en_top100k.vec', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 301:\n",
    "            word = parts[0]\n",
    "            vec = list(map(float, parts[1:]))\n",
    "            en_vectors[word] = vec\n",
    "\n",
    "# Load Hindi embeddings into a dictionary\n",
    "hi_vectors = {}\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/fasttext_pretrained_vectors/hi_top100k.vec', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 301:\n",
    "            word = parts[0]\n",
    "            vec = list(map(float, parts[1:]))\n",
    "            hi_vectors[word] = vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Matrices X and Y from train_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vectors from train_pairs\n",
    "X = []  # English vectors\n",
    "Y = []  # Hindi vectors\n",
    "\n",
    "for en_word, hi_word in train_pairs:\n",
    "    if en_word in en_vectors and hi_word in hi_vectors:\n",
    "        X.append(en_vectors[en_word])\n",
    "        Y.append(hi_vectors[hi_word])\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(\"X shape:\", X.shape)  # Should be (N, 300)\n",
    "print(\"Y shape:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procrustes Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each row vector in X and Y\n",
    "from numpy.linalg import norm\n",
    "\n",
    "X_norm = X / norm(X, axis=1, keepdims=True)\n",
    "Y_norm = Y / norm(Y, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Procrustes Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Procrustes alignment using SVD\n",
    "import scipy\n",
    "\n",
    "# Compute cross-covariance matrix\n",
    "M = Y_norm.T @ X_norm\n",
    "\n",
    "# SVD decomposition\n",
    "U, _, Vt = np.linalg.svd(M)\n",
    "\n",
    "# Compute the orthogonal matrix W\n",
    "W = U @ Vt\n",
    "\n",
    "# Align Hindi embeddings\n",
    "Y_aligned = Y_norm @ W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Orthogonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W should be orthogonal: W.T @ W ‚âà Identity matrix\n",
    "check = W.T @ W\n",
    "print(\"Should be close to identity:\")\n",
    "print(check[:5, :5])  # Just printing a small corner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Test Dictionary Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test word pairs (created earlier)\n",
    "test_pairs = []\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/data/raw_data/test_pairs.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            test_pairs.append((parts[0], parts[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Aligned Hindi Vocabulary Matrix for Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a lookup matrix and list of words for nearest neighbor search\n",
    "aligned_hindi_words = list(hi_vectors.keys())  # Already filtered in Task 1\n",
    "aligned_hindi_vecs = []\n",
    "\n",
    "for word in aligned_hindi_words:\n",
    "    if word in hi_vectors:\n",
    "        vec = np.array(hi_vectors[word])\n",
    "        vec = vec / np.linalg.norm(vec)  # normalize\n",
    "        aligned_vec = vec @ W            # apply Procrustes transform\n",
    "        aligned_hindi_vecs.append(aligned_vec)\n",
    "\n",
    "aligned_hindi_vecs = np.array(aligned_hindi_vecs)  # shape (V, 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation & Precision Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top1_correct = 0\n",
    "top5_correct = 0\n",
    "total = 0\n",
    "\n",
    "for en_word, true_hi_word in test_pairs:\n",
    "    if en_word not in en_vectors or true_hi_word not in hi_vectors:\n",
    "        continue\n",
    "\n",
    "    en_vec = np.array(en_vectors[en_word])\n",
    "    en_vec = en_vec / np.linalg.norm(en_vec)\n",
    "\n",
    "    similarities = cosine_similarity(en_vec.reshape(1, -1), aligned_hindi_vecs)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1]  # sorted high ‚Üí low\n",
    "\n",
    "    top_5_words = [aligned_hindi_words[i] for i in top_indices[:5]]\n",
    "\n",
    "    total += 1\n",
    "    if true_hi_word == top_5_words[0]:\n",
    "        top1_correct += 1\n",
    "    if true_hi_word in top_5_words:\n",
    "        top5_correct += 1\n",
    "\n",
    "# Compute precision\n",
    "precision_at_1 = top1_correct / total\n",
    "precision_at_5 = top5_correct / total\n",
    "\n",
    "print(\"Precision@1:\", round(precision_at_1, 4))\n",
    "print(\"Precision@5:\", round(precision_at_5, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity for a few matched pairs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = []\n",
    "for en_word, hi_word in random.sample(test_pairs, 100):  # sample 100\n",
    "    if en_word in en_vectors and hi_word in hi_vectors:\n",
    "        en_vec = np.array(en_vectors[en_word])\n",
    "        hi_vec = np.array(hi_vectors[hi_word])\n",
    "        hi_vec = hi_vec / np.linalg.norm(hi_vec)\n",
    "        hi_vec = hi_vec @ W\n",
    "        en_vec = en_vec / np.linalg.norm(en_vec)\n",
    "        sim = np.dot(en_vec, hi_vec)\n",
    "        scores.append(sim)\n",
    "\n",
    "plt.hist(scores, bins=20)\n",
    "plt.title(\"Cosine Similarity Between True Word Pairs\")\n",
    "plt.xlabel(\"Similarity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of English words you want to test\n",
    "words_to_test = [\"water\", \"king\", \"love\", \"book\", \"mother\", \"school\"]\n",
    "\n",
    "for en_word in words_to_test:\n",
    "    print(\"üî§ English word:\", en_word)\n",
    "\n",
    "    if en_word not in en_vectors:\n",
    "        print(\"   ‚ùå Not found in English vocabulary.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Get and normalize English vector\n",
    "    en_vec = np.array(en_vectors[en_word])\n",
    "    en_vec = en_vec / np.linalg.norm(en_vec)\n",
    "\n",
    "    # Compute cosine similarity with all aligned Hindi vectors\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(en_vec.reshape(1, -1), aligned_hindi_vecs)[0]\n",
    "\n",
    "    # Get top 5 closest Hindi words\n",
    "    top_indices = np.argsort(similarities)[::-1][:5]\n",
    "    \n",
    "    print(\"üåê Top 5 Hindi translations:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        print(f\"   {i+1}. {aligned_hindi_words[idx]}\")\n",
    "    \n",
    "    print(\"\")  # Blank line between results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation Study ‚Äì Impact of Dictionary Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Multiple Training Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the earlier shuffled valid_pairs list (used in train/test split)\n",
    "subset_5k = train_pairs[:5000]\n",
    "subset_10k = train_pairs[:10000]\n",
    "subset_20k = train_pairs[:20000]\n",
    "\n",
    "# Save to files\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/evaluation/precision_scores/train_5k.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, hi in subset_5k:\n",
    "        f.write(f\"{en} {hi}\\n\")\n",
    "\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/evaluation/precision_scores/train_10k.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, hi in subset_10k:\n",
    "        f.write(f\"{en} {hi}\\n\")\n",
    "\n",
    "with open('/Users/ayushyapare/Desktop/Ayushyas_Life/Work/Projects/Sarvam/Cross-Lingual-Word-Embedding-Alignment/evaluation/precision_scores/train_20k.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, hi in subset_20k:\n",
    "        f.write(f\"{en} {hi}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop Through Each Set and Record P@1, P@5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary sizes to test\n",
    "sizes = [5000, 10000, 20000]\n",
    "\n",
    "# Store results for comparison\n",
    "results = []\n",
    "\n",
    "for size in sizes:\n",
    "    print(\"üîÅ Running ablation for size:\", size)\n",
    "\n",
    "    # Load current training dictionary\n",
    "    current_train_file = f'train_{size//1000}k.txt'\n",
    "    current_train_pairs = []\n",
    "\n",
    "    with open(current_train_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 2:\n",
    "                current_train_pairs.append((parts[0], parts[1]))\n",
    "\n",
    "    # Build X and Y matrices\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for en_word, hi_word in current_train_pairs:\n",
    "        if en_word in en_vectors and hi_word in hi_vectors:\n",
    "            X.append(en_vectors[en_word])\n",
    "            Y.append(hi_vectors[hi_word])\n",
    "\n",
    "    import numpy as np\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Normalize\n",
    "    from numpy.linalg import norm\n",
    "    X_norm = X / norm(X, axis=1, keepdims=True)\n",
    "    Y_norm = Y / norm(Y, axis=1, keepdims=True)\n",
    "\n",
    "    # Procrustes alignment\n",
    "    M = Y_norm.T @ X_norm\n",
    "    U, _, Vt = np.linalg.svd(M)\n",
    "    W = U @ Vt\n",
    "    Y_aligned = Y_norm @ W\n",
    "\n",
    "    # Evaluate on test_pairs\n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "    total = 0\n",
    "\n",
    "    # Rebuild aligned Hindi vocab matrix (only once per run)\n",
    "    aligned_hindi_words = list(hi_vectors.keys())\n",
    "    aligned_hindi_vecs = []\n",
    "\n",
    "    for word in aligned_hindi_words:\n",
    "        vec = np.array(hi_vectors[word])\n",
    "        vec = vec / np.linalg.norm(vec)\n",
    "        aligned_vec = vec @ W\n",
    "        aligned_hindi_vecs.append(aligned_vec)\n",
    "\n",
    "    aligned_hindi_vecs = np.array(aligned_hindi_vecs)\n",
    "\n",
    "    # Evaluate test_pairs\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "    for en_word, true_hi_word in test_pairs:\n",
    "        if en_word not in en_vectors or true_hi_word not in hi_vectors:\n",
    "            continue\n",
    "\n",
    "        en_vec = np.array(en_vectors[en_word])\n",
    "        en_vec = en_vec / np.linalg.norm(en_vec)\n",
    "        similarities = cosine_similarity(en_vec.reshape(1, -1), aligned_hindi_vecs)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:5]\n",
    "        top_words = [aligned_hindi_words[i] for i in top_indices]\n",
    "\n",
    "        total += 1\n",
    "        if true_hi_word == top_words[0]:\n",
    "            top1 += 1\n",
    "        if true_hi_word in top_words:\n",
    "            top5 += 1\n",
    "\n",
    "    # Calculate precision\n",
    "    p1 = top1 / total\n",
    "    p5 = top5 / total\n",
    "\n",
    "    results.append((size, p1, p5))\n",
    "\n",
    "    print(f\"‚úÖ Size {size}: P@1 = {round(p1, 4)}, P@5 = {round(p5, 4)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    (5000, 0.2485, 0.4872),\n",
    "    (10000, 0.2593, 0.5051),\n",
    "    (20000, 0.2664, 0.5159)\n",
    "]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract data\n",
    "sizes = [r[0] for r in results]\n",
    "p1_scores = [r[1] for r in results]\n",
    "p5_scores = [r[2] for r in results]\n",
    "\n",
    "x = range(len(sizes))\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "bar1 = plt.bar([i - 0.2 for i in x], p1_scores, width=0.4, label='Precision@1')\n",
    "bar2 = plt.bar([i + 0.2 for i in x], p5_scores, width=0.4, label='Precision@5')\n",
    "\n",
    "# Add labels\n",
    "plt.xticks(x, [f\"{s//1000}k\" for s in sizes])\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.xlabel(\"Training Dictionary Size\")\n",
    "plt.title(\"Ablation Study: Dictionary Size vs Translation Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "# Show values on top of bars\n",
    "for i in range(len(sizes)):\n",
    "    plt.text(i - 0.2, p1_scores[i] + 0.01, f\"{p1_scores[i]:.2f}\", ha='center')\n",
    "    plt.text(i + 0.2, p5_scores[i] + 0.01, f\"{p5_scores[i]:.2f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "constructor",
   "language": "python",
   "name": "constructor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
